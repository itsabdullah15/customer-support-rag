{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "018af611",
   "metadata": {},
   "source": [
    "## Load JSON → SPLIT → EMBED → SAVE PINECONE (Vector DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aa447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Processing: angelone_quick_10_links_support_data.json\n",
      "✅ Loaded 10 entries\n",
      "\n",
      "📄 Processing: angelone_support_full_data.json\n",
      "✅ Loaded 17 entries\n",
      "\n",
      "📄 Processing: insurance_pdfs_flat.json\n",
      "✅ Loaded 26 entries\n",
      "\n",
      "📊 Total Chunks Created: 462\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# 📁 Folder containing JSON files (each with a list of PDF pages)\n",
    "folder_path = os.path.abspath(\"../01_data_gathering_logic\")\n",
    "\n",
    "# 🧠 Text splitter configuration\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=200)\n",
    "\n",
    "# 📚 All resulting Document objects\n",
    "all_documents = []\n",
    "\n",
    "# 🔁 Process each JSON file\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"\\n📄 Processing: {filename}\")\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                entries = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {filename} | Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"✅ Loaded {len(entries)} entries\")\n",
    "\n",
    "        for entry in entries:\n",
    "            pdf_name = entry.get(\"Text\", filename)\n",
    "            page_number = entry.get(\"page_number\", None)\n",
    "            raw_text = entry.get(\"Details\", \"\").strip()\n",
    "\n",
    "            if not raw_text:\n",
    "                continue\n",
    "\n",
    "            chunks = splitter.split_text(raw_text)\n",
    "\n",
    "            for chunk in chunks:\n",
    "                doc = Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        \"source\": pdf_name,\n",
    "                        \"page_number\": page_number\n",
    "                    }\n",
    "                )\n",
    "                all_documents.append(doc)\n",
    "\n",
    "print(f\"\\n📊 Total Chunks Created: {len(all_documents)}\")\n",
    "\n",
    "# Optional: Preview first few\n",
    "# for i, doc in enumerate(all_documents[:3], 1):\n",
    "#     print(f\"\\n🔹 Chunk {i}:\")\n",
    "#     print(f\"Metadata: {doc.metadata}\")\n",
    "#     print(f\"Content: {doc.page_content[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879e025",
   "metadata": {},
   "source": [
    "Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b98a420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\abdullah.shahid\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔄 Generating embeddings: 100%|██████████| 15/15 [00:07<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Generated 462 embeddings of dimension 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import List, Optional\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_embeddings(\n",
    "    texts: List[str],\n",
    "    model_name: str = \"all-MiniLM-L6-v2\",\n",
    "    batch_size: int = 32,\n",
    "    device: Optional[str] = None  # e.g., \"cuda\" or \"cpu\"\n",
    ") -> np.ndarray:\n",
    "    if not texts:\n",
    "        print(\"⚠️ No input texts provided.\")\n",
    "        return np.array([])\n",
    "\n",
    "    # Load the model on the specified device\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"🔄 Generating embeddings\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "\n",
    "        try:\n",
    "            batch_embeddings = model.encode(\n",
    "                batch,\n",
    "                show_progress_bar=False,\n",
    "                convert_to_numpy=True,\n",
    "                normalize_embeddings=True  # Optional: normalize for cosine similarity\n",
    "            )\n",
    "            embeddings.append(batch_embeddings)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in batch {i // batch_size}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return np.vstack(embeddings) if embeddings else np.array([])\n",
    "\n",
    "# Example usage:\n",
    "texts = [doc.page_content for doc in all_documents]\n",
    "embeddings = generate_embeddings(texts)\n",
    "\n",
    "if embeddings.size > 0:\n",
    "    print(f\"\\n✅ Generated {embeddings.shape[0]} embeddings of dimension {embeddings.shape[1]}\")\n",
    "else:\n",
    "    print(\"❌ Failed to generate embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb12c8",
   "metadata": {},
   "source": [
    "Pinecone Vector DB | Saving the data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2ddd3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Index already exists: vector-search-index\n",
      "⬆️ Upserted batch 1: 100 vectors\n",
      "⬆️ Upserted batch 2: 100 vectors\n",
      "⬆️ Upserted batch 3: 100 vectors\n",
      "⬆️ Upserted batch 4: 100 vectors\n",
      "⬆️ Upserted batch 5: 62 vectors\n",
      "✅ Indexed 462 vectors.\n",
      "💾 Saved texts to documents.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pinecone\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "if not PINECONE_API_KEY:\n",
    "    raise EnvironmentError(\"Missing PINECONE_API_KEY in environment variables.\")\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pinecone_client = pinecone.Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Pinecone config\n",
    "INDEX_NAME = \"vector-search-index\"\n",
    "DIMENSION = 384\n",
    "METRIC = \"cosine\"\n",
    "REGION = \"us-east-1\"\n",
    "CLOUD = \"aws\"\n",
    "\n",
    "\n",
    "def create_index_if_not_exists() -> None:\n",
    "    \"\"\"Create Pinecone index if it doesn't exist.\"\"\"\n",
    "    try:\n",
    "        if INDEX_NAME not in pinecone_client.list_indexes().names():\n",
    "            pinecone_client.create_index(\n",
    "                name=INDEX_NAME,\n",
    "                dimension=DIMENSION,\n",
    "                metric=METRIC,\n",
    "                spec=pinecone.ServerlessSpec(cloud=CLOUD, region=REGION),\n",
    "            )\n",
    "            print(f\"✅ Created index: {INDEX_NAME}\")\n",
    "        else:\n",
    "            print(f\"ℹ️ Index already exists: {INDEX_NAME}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error creating index: {e}\")\n",
    "\n",
    "\n",
    "def get_index():\n",
    "    \"\"\"Retrieve Pinecone index object.\"\"\"\n",
    "    try:\n",
    "        return pinecone_client.Index(INDEX_NAME)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error connecting to index: {e}\")\n",
    "\n",
    "\n",
    "def index_embeddings(embeddings: np.ndarray, texts: List[str], batch_size: int = 100) -> None:\n",
    "    \"\"\"Upsert vectors and associate them with text IDs.\"\"\"\n",
    "    if len(embeddings) != len(texts):\n",
    "        raise ValueError(\"Embeddings and texts length mismatch.\")\n",
    "\n",
    "    index = get_index()\n",
    "\n",
    "    for i in range(0, len(embeddings), batch_size):\n",
    "        batch_ids = [str(i + j) for j in range(min(batch_size, len(embeddings) - i))]\n",
    "        batch_vectors = embeddings[i:i + batch_size]\n",
    "        upsert_data = [(id_, vec.tolist()) for id_, vec in zip(batch_ids, batch_vectors)]\n",
    "\n",
    "        index.upsert(vectors=upsert_data)\n",
    "        print(f\"⬆️ Upserted batch {i // batch_size + 1}: {len(upsert_data)} vectors\")\n",
    "\n",
    "    print(f\"✅ Indexed {len(embeddings)} vectors.\")\n",
    "\n",
    "\n",
    "def save_texts_to_json(texts: List[str], file_path: str = \"documents.json\") -> None:\n",
    "    \"\"\"Save text data to JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(texts, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"💾 Saved texts to {file_path}\")\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error saving texts: {e}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # embeddings = np.random.rand(10, DIMENSION)  # Replace with actual embeddings\n",
    "    # texts = [f\"Text chunk {i}\" for i in range(10)]\n",
    "\n",
    "    create_index_if_not_exists()\n",
    "    index_embeddings(embeddings, texts)\n",
    "    save_texts_to_json(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a94417",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
