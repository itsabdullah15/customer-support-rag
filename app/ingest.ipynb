{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fb210a2",
   "metadata": {},
   "source": [
    "# Load JSON ‚Üí split ‚Üí embed ‚Üí save FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "561408a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Processing file: angelone_quick_10_links_support_data.json\n",
      "‚úÖ Loaded 10 entries\n",
      "\n",
      "üìÑ Processing file: angelone_support_full_data.json\n",
      "‚úÖ Loaded 17 entries\n",
      "\n",
      "üìÑ Processing file: insurance_pdfs_text.json\n",
      "‚úÖ Loaded 26 entries\n",
      "\n",
      "üîç Total Chunks Created: 1840\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Folder containing JSON files\n",
    "folder_path = r\"C:\\Users\\abdullah.shahid\\Desktop\\Python\\2025\\April-May\\customer-support-rag\\data\"\n",
    "\n",
    "# Configure text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=100)\n",
    "\n",
    "# Store all documents\n",
    "all_documents = []\n",
    "\n",
    "# Process each JSON file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"\\nüìÑ Processing file: {filename}\")\n",
    "\n",
    "        # Load JSON\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        print(f\"‚úÖ Loaded {len(data)} entries\")\n",
    "\n",
    "        # Process each entry in the JSON\n",
    "        for entry in data:\n",
    "            text = entry.get(\"Text\", \"\")  # You can rename this to \"pdf_name\" if that matches your schema\n",
    "            details = entry.get(\"Details\", \"\").strip()\n",
    "\n",
    "            if not details.strip():\n",
    "                continue  # Skip empty entries\n",
    "\n",
    "            # Split details into chunks\n",
    "            chunks = splitter.split_text(details)\n",
    "\n",
    "            # Create Document objects\n",
    "            for chunk in chunks:\n",
    "                all_documents.append(Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\"text\": text}\n",
    "                ))\n",
    "\n",
    "print(f\"\\nüîç Total Chunks Created: {len(all_documents)}\")\n",
    "\n",
    "# Preview the first chunk (optional)\n",
    "# if all_documents:\n",
    "    # preview = all_documents[0].page_content[:200].replace('\\n', ' ')\n",
    "    # print(f\"üìù Sample Chunk Preview: {preview}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7bc27b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58/58 [00:35<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1840 embeddings with 384 dimensions each.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm  # For showing progress bars during batch processing\n",
    "\n",
    "def generate_embeddings(texts, model_name=\"all-MiniLM-L6-v2\", batch_size=32):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Store embeddings in a list (will convert to numpy array later)\n",
    "    embeddings = []\n",
    "    \n",
    "    # Process texts in batches\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        try:\n",
    "            # Generate embeddings for the current batch\n",
    "            batch_embeddings = model.encode(batch, show_progress_bar=False)\n",
    "            embeddings.append(batch_embeddings)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i // batch_size}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Flatten the list of embeddings and return as numpy array\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Extract text chunks from LangChain Document objects\n",
    "texts = [doc.page_content for doc in all_documents]\n",
    "\n",
    "# Generate embeddings with improved batch processing\n",
    "embeddings = generate_embeddings(texts)\n",
    "\n",
    "print(f\"Generated {embeddings.shape[0]} embeddings with {embeddings.shape[1]} dimensions each.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfd86279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved to faiss_index.index\n",
      "Text data saved to documents.json\n",
      "FAISS index contains 1840 vectors.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Step 1: Create FAISS Index\n",
    "def create_faiss_index(embeddings):\n",
    "    # Embedding dimension (depends on the model)\n",
    "    dim = embeddings.shape[1]\n",
    "    \n",
    "    # Create the FAISS index (L2 distance metric)\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    \n",
    "    # Add embeddings to the index\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index\n",
    "\n",
    "# Step 2: Save FAISS Index to Disk\n",
    "def save_faiss_index(index, index_file=\"faiss_index.index\"):\n",
    "    faiss.write_index(index, index_file)\n",
    "    print(f\"FAISS index saved to {index_file}\")\n",
    "\n",
    "# Step 3: Save Text Data to Disk\n",
    "def save_texts(texts, text_file=\"documents.json\"):\n",
    "    with open(text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(texts, f, ensure_ascii=False)\n",
    "    print(f\"Text data saved to {text_file}\")\n",
    "\n",
    "\n",
    "faiss_index = create_faiss_index(embeddings)\n",
    "save_faiss_index(faiss_index)\n",
    "save_texts(texts)\n",
    "\n",
    "print(f\"FAISS index contains {faiss_index.ntotal} vectors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7ff5534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'vector-search-index' already exists.\n",
      "Upserted batch 1 (10 vectors).\n",
      "Successfully indexed 10 vectors.\n",
      "Text data saved to documents.json.\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Any\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get Pinecone API key\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\"PINECONE_API_KEY not found in environment variables.\")\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pinecone_client = pinecone.Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Index configuration\n",
    "INDEX_NAME = \"vector-search-index\"\n",
    "DIMENSION = 384  # Matches embedding model (e.g., all-MiniLM-L6-v2)\n",
    "METRIC = \"cosine\"  # Cosine similarity for semantic search\n",
    "\n",
    "# Check if index exists, create if it doesn't\n",
    "def create_pinecone_index():\n",
    "    try:\n",
    "        if INDEX_NAME not in pinecone_client.list_indexes().names():\n",
    "            pinecone_client.create_index(\n",
    "                name=INDEX_NAME,\n",
    "                dimension=DIMENSION,\n",
    "                metric=METRIC,\n",
    "                spec=pinecone.ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "            )\n",
    "            print(f\"Created Pinecone index '{INDEX_NAME}'.\")\n",
    "        else:\n",
    "            print(f\"Index '{INDEX_NAME}' already exists.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to create Pinecone index: {e}\")\n",
    "\n",
    "# Connect to the Pinecone index\n",
    "def get_pinecone_index():\n",
    "    try:\n",
    "        return pinecone_client.Index(INDEX_NAME)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to connect to Pinecone index '{INDEX_NAME}': {e}\")\n",
    "\n",
    "# Step 1: Index Vectors into Pinecone\n",
    "def index_vectors(embeddings: np.ndarray, texts: List[str], batch_size: int = 100) -> None:\n",
    "    try:\n",
    "        index = get_pinecone_index()\n",
    "        total_vectors = len(embeddings)\n",
    "        \n",
    "        # Ensure embeddings and texts have the same length\n",
    "        if len(embeddings) != len(texts):\n",
    "            raise ValueError(\"Number of embeddings and texts must match.\")\n",
    "        \n",
    "        # Batch upsert to handle large datasets\n",
    "        for i in range(0, total_vectors, batch_size):\n",
    "            batch_embeddings = embeddings[i:i + batch_size]\n",
    "            batch_ids = [str(j) for j in range(i, min(i + batch_size, total_vectors))]\n",
    "            to_upsert = [(id_, vec.tolist()) for id_, vec in zip(batch_ids, batch_embeddings)]\n",
    "            index.upsert(vectors=to_upsert)\n",
    "            print(f\"Upserted batch {i // batch_size + 1} ({len(batch_embeddings)} vectors).\")\n",
    "        \n",
    "        print(f\"Successfully indexed {total_vectors} vectors.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to index vectors: {e}\")\n",
    "\n",
    "# Step 2: Save Text Data to Disk\n",
    "def save_texts(texts: List[str], text_file: str = \"documents.json\") -> None:\n",
    "    try:\n",
    "        with open(text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(texts, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Text data saved to {text_file}.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to save texts to {text_file}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data (replace with your actual embeddings and texts)\n",
    "    # embeddings = np.random.rand(10, DIMENSION)  # Mock embeddings (10 vectors of 384 dims)\n",
    "    # texts = [f\"Document {i}\" for i in range(10)]  # Mock text data\n",
    "\n",
    "    # Create index if it doesn't exist\n",
    "    create_pinecone_index()\n",
    "\n",
    "    # Index vectors and save texts\n",
    "    index_vectors(embeddings, texts)\n",
    "    save_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0a881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
